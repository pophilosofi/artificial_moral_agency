# artificial_moral_agency
Research Thesis: Possibility of Extending Moral Agency to Artificial Agents 

Abstract— Artificial Intelligence (A.I.) constitutes various aspect of modern life, from Machine Learning algorithms predicting stocks to the killing of belligerents, and innocents alike on the battlefield. Moreover, the end goal is to create autonomous A.I. This means that the presence of humans in the decision-making process will be absent. However, when an A.I. does something wrong and its actions go against the law; who is responsible?

This research’s subject matter in A.I. and Robot Ethics focused mainly on Robot Rights and its ultimate objective was to answer the questions: (i) What is the function of rights?(ii) Who is a right holder, what is personhood and the requirements needed to be a moral agent (therefore, accountable for responsibility)?; (iii) Can an A.I. be a moral agent? (ontological requirements) and finally (iv) if then it ought to be one (ethical implications).
With the goal to answer these questions this research project set out to find a plausible solution for the accountability of A.I. All while avoiding drawing argumentation from a distant future where A.I is super intelligent but instead working with the present-day technologies and how they could interact with real world legal systems and philosophical categories. 

It was found that all rights are positive and based on consensus, they change with time based on circumstances. Their function is to protect the social fabric. The same goes for the requirements considered necessary to be a moral agent: those are not absolute; in fact, they are constantly redesigned. 
The next logical step was to identify what requirements are regarded as fundamental in real-world judicial systems, comparing them to that of ones used in philosophy. Autonomy, free will, intentionality, consciousness and responsibility were identified as the requirements to be considered a moral agent. A symmetrical system between personhood and A.I. was built to enable the emergence of the ontological differences between the two. Each requirement was introduced, explained in the most relevant theories of contemporary philosophy, and observed in its manifestation in A.I.

After completing the philosophical and technical analysis, conclusions were drawn. There are two issues regarding the assignment of moral agency to artificial agent: (i) the ontological requirements must be present; and (ii) whether an A.I. ought to be considered as an artificial moral agent. From an ontological point of view it was difficult to prove that an A.I. could be autonomous, free, intentional, conscious, and responsible. The philosophical accounts are often very theorical and inconclusive, making it difficult to fully detect these requirements on an experimental level of demonstration. However, from an ethical point of view it makes sense to consider some A.I. as artificial moral agents. When considering artificial agents as responsible, there can be applied already existing norms in our judicial system such as removing them from society, and re-educating them, to re-introduce them to society, in line with how correctional facilities ought to work. 
